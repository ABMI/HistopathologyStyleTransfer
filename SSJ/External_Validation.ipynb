{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Validation - GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some packages to use\n",
    "import cv2 # OpenCV; 이미지 처리\n",
    "import numpy as np # Mathematical lib.; 다차원 배열과 매트릭스의 빠른 계산 지원\n",
    "import pandas as pd # Data manipulation and analysis; 특히 숫자표와 시계열 데이터 조작\n",
    "\n",
    "import matplotlib.pyplot as plt # Plotting & Image-displaying\n",
    "%matplotlib inline \n",
    "# Notebook에 plot을 표시\n",
    "\n",
    "# To see our directory\n",
    "import os # computer/file 시스템에 접근\n",
    "import random # data set split&shuffle\n",
    "import gc # 불필요한 변수 및 메모리 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "#os.chdir('/home/ssj0921/SSJ/Data/20190731_Classification/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/ssj0921/SSJ/Data/20191203_Classification/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "sample_9056_df = pd.read_csv('./sample_2000_df_withoutJPG.csv')\n",
    "sample_9056_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('non-cancer sample (sample_9056_df): ', len(sample_9056_df[sample_9056_df['label'] == 0]))\n",
    "print('cancer sample (sample_9056_df)    : ', len(sample_9056_df[sample_9056_df['label'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_9056_df_Cancer = sample_9056_df[sample_9056_df['label'] == 1]\n",
    "sample_9056_df_nonCancer = sample_9056_df[sample_9056_df['label'] == 0]\n",
    "\n",
    "sample_9056_df_Cancer_500 = sample_9056_df_Cancer.sample(n=500)\n",
    "sample_9056_df_nonCancer_500 = sample_9056_df_nonCancer.sample(n=500)\n",
    "\n",
    "len(sample_9056_df_Cancer_500), len(sample_9056_df_nonCancer_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_9056_df = sample_9056_df_Cancer_500.append(sample_9056_df_nonCancer_500)\n",
    "len(sample_9056_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_A_list = []\n",
    "for i in range(len(sample_9056_df)):\n",
    "    withoutJPG = sample_9056_df.filename.values[i] + '_real_A.png'\n",
    "    real_A_list.append(withoutJPG)    \n",
    "\n",
    "real_A_df = pd.DataFrame(data={'filename': real_A_list, 'label': sample_9056_df.label.values.tolist()})    \n",
    "len(real_A_df), real_A_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('non-cancer sample (real_A): ', len(real_A_df[real_A_df['label'] == 0]))\n",
    "print('cancer sample (real_A)    : ', len(real_A_df[real_A_df['label'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_B_list = []\n",
    "for i in range(len(sample_9056_df)):\n",
    "    withoutJPG = sample_9056_df.filename.values[i] + '_fake_B.png'\n",
    "    fake_B_list.append(withoutJPG)    \n",
    "\n",
    "fake_B_df = pd.DataFrame(data={'filename': fake_B_list, 'label': sample_9056_df.label.values.tolist()})    \n",
    "len(fake_B_df), fake_B_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('non-cancer sample (fake_B): ', len(fake_B_df[fake_B_df['label'] == 0]))\n",
    "print('cancer sample (fake_B)    : ', len(fake_B_df[fake_B_df['label'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib을 이용하여 df의 파일 경로로부터 Image 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.image import imread\n",
    "\n",
    "ext_im_list_real_A = []\n",
    "\n",
    "for filename in real_A_df['filename']:    \n",
    "    path = '/home/ssj0921/SSJ/Data/20191203_Classification/20191203_real_A_2000/' + filename\n",
    "    img = imread(path)\n",
    "    ext_im_list_real_A.append(img)\n",
    "len(ext_im_list_real_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib을 이용하여 df의 파일 경로로부터 Image 읽기\n",
    "from matplotlib.image import imread\n",
    "\n",
    "ext_im_list_fake_B = []\n",
    "\n",
    "for filename in fake_B_df['filename']:    \n",
    "    path = '/home/ssj0921/SSJ/Data/20191203_Classification/20191203_fake_B_2000/' + filename\n",
    "    img = imread(path)\n",
    "    ext_im_list_fake_B.append(img)\n",
    "len(ext_im_list_fake_B)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate Color Image View\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def showImage(img):\n",
    "    plt.imshow(img)    \n",
    "    plt.show()\n",
    "\n",
    "def showImages(input_img):\n",
    "    f, axarr = plt.subplots(2,2)\n",
    "\n",
    "    axarr[0,0].imshow(input_img[...,0]) # R\n",
    "    axarr[0,1].imshow(input_img[...,1]) # G\n",
    "    axarr[1,0].imshow(input_img[...,2]) # B\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ext_image_real_A = np.array(ext_im_list_real_A) \n",
    "ext_file_real_A = np.array(real_A_df['filename']) \n",
    "ext_label_real_A = np.array(real_A_df['label']) \n",
    "\n",
    "print('<real_A>')\n",
    "print('ext_image: ', len(ext_image_real_A), type(ext_image_real_A), ext_image_real_A.shape)  # 9,557\n",
    "print('ext_file:  ', len(ext_file_real_A), type(ext_file_real_A), ext_file_real_A.shape)\n",
    "print('ext_label: ', len(ext_label_real_A), type(ext_label_real_A), ext_label_real_A.shape)\n",
    "\n",
    "ext_image_fake_B = np.array(ext_im_list_fake_B) \n",
    "ext_file_fake_B = np.array(fake_B_df['filename']) \n",
    "ext_label_fake_B = np.array(fake_B_df['label']) \n",
    "\n",
    "print('<fake_B>')\n",
    "print('ext_image: ', len(ext_image_fake_B), type(ext_image_fake_B), ext_image_fake_B.shape)  # 9,557\n",
    "print('ext_file:  ', len(ext_file_fake_B), type(ext_file_fake_B), ext_file_fake_B.shape)\n",
    "print('ext_label: ', len(ext_label_fake_B), type(ext_label_fake_B), ext_label_fake_B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "def visualize_data(cancer_images, normal_images):\n",
    "    # INPUTS\n",
    "    # positive_images - Images where the label = 1 (True)\n",
    "    # negative_images - Images where the label = 0 (False)\n",
    " \n",
    "    figure = plt.figure()\n",
    "    count = 0\n",
    " \n",
    "    for i in range(cancer_images.shape[0]):\n",
    "        count += 1\n",
    "        figure.add_subplot(2, cancer_images.shape[0], count)\n",
    "        plt.imshow(cancer_images[i, :, :])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Positive\".format(i))\n",
    " \n",
    "        figure.add_subplot(1, normal_images.shape[0], count)\n",
    "        plt.imshow(normal_images[i, :, :])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Negative\".format(i), y=-0.3)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of positive and negative examples to show\n",
    "N_TO_VISUALIZE = 5\n",
    " \n",
    "# Select the first N Cancer examples\n",
    "ext_positive_example_indices = (ext_label_real_A == 1)\n",
    "ext_positive_examples = ext_image_real_A[ext_positive_example_indices, :, :]\n",
    "ext_positive_examples = ext_positive_examples[0:N_TO_VISUALIZE, :, :]\n",
    " \n",
    "# Select the first N Normal examples\n",
    "ext_negative_example_indices = (ext_label_real_A == 0)\n",
    "ext_negative_examples = ext_image_real_A[ext_negative_example_indices, :, :]\n",
    "ext_negative_examples = ext_negative_examples[0:N_TO_VISUALIZE, :, :]\n",
    " \n",
    "# Call the visualization function\n",
    "visualize_data(ext_positive_examples, ext_negative_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of positive and negative examples to show\n",
    "N_TO_VISUALIZE = 5\n",
    " \n",
    "# Select the first N Cancer examples\n",
    "ext_positive_example_indices = (ext_label_fake_B == 1)\n",
    "ext_positive_examples = ext_image_fake_B[ext_positive_example_indices, :, :]\n",
    "ext_positive_examples = ext_positive_examples[0:N_TO_VISUALIZE, :, :]\n",
    " \n",
    "# Select the first N Normal examples\n",
    "ext_negative_example_indices = (ext_label_fake_B == 0)\n",
    "ext_negative_examples = ext_image_fake_B[ext_negative_example_indices, :, :]\n",
    "ext_negative_examples = ext_negative_examples[0:N_TO_VISUALIZE, :, :]\n",
    " \n",
    "# Call the visualization function\n",
    "visualize_data(ext_positive_examples, ext_negative_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] Image resize and data set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Resize Image\n",
    "ext_image_299_real_A = []\n",
    "\n",
    "for img in ext_image_real_A:\n",
    "    # arr = Image.fromarray(img)  # jpg\n",
    "    arr = Image.fromarray((img*255).astype(np.uint8))  # png (cGAN)\n",
    "    res = arr.resize((299, 299))\n",
    "    ext_image_299_real_A.append(np.array(res))\n",
    "    \n",
    "ext_image_299_real_A = np.array(ext_image_299_real_A)   \n",
    "len(ext_image_299_real_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Resize Image\n",
    "ext_image_299_fake_B = []\n",
    "\n",
    "for img in ext_image_fake_B:\n",
    "    # arr = Image.fromarray(img)  # jpg\n",
    "    arr = Image.fromarray((img*255).astype(np.uint8))  # png (cGAN)\n",
    "    res = arr.resize((299, 299))\n",
    "    ext_image_299_fake_B.append(np.array(res))\n",
    "    \n",
    "ext_image_299_fake_B = np.array(ext_image_299_fake_B)    \n",
    "len(ext_image_299_fake_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize\n",
    "ext_image_299_real_A = ext_image_299_real_A / 255\n",
    "ext_image_299_fake_B = ext_image_299_fake_B / 255\n",
    "len(ext_image_299_real_A), len(ext_image_299_fake_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('/home/ssj0921/SSJ/Model/Model_Class_256_InceptionV3_E30_TCIA_20191101.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터로 평가하기\n",
    "score_real_A = model.evaluate(ext_image_299_real_A, ext_label_real_A, verbose=1)\n",
    "print('loss=', score_real_A[0])\n",
    "print('accuracy=', score_real_A[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve\n",
    "\n",
    "ext_predict_score_real_A = model.predict_proba(ext_image_299_real_A, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict crisp classes for test set\n",
    "ext_predict_classes_real_A = model.predict_classes(ext_image_299_real_A, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fake_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터로 평가하기\n",
    "score_fake_B = model.evaluate(ext_image_299_fake_B, ext_label_fake_B, verbose=1)\n",
    "print('loss=', score_fake_B[0])\n",
    "print('accuracy=', score_fake_B[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve\n",
    "\n",
    "ext_predict_score_fake_B = model.predict_proba(ext_image_299_fake_B, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict crisp classes for test set\n",
    "ext_predict_classes_fake_B = model.predict_classes(ext_image_299_fake_B, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### before-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images_before = ext_image_299_real_A\n",
    "Images_file_before = ext_file_real_A\n",
    "True_label_before = ext_label_real_A\n",
    "Predic_score_before = ext_predict_score_real_A\n",
    "Predic_class_before = ext_predict_classes_real_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images_before.shape, Images_file_before.shape, True_label_before.shape, Predic_score_before.shape, Predic_class_before.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predic_score_before.shape = (1000,)\n",
    "Predic_class_before.shape = (1000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### after-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images_after = ext_image_299_fake_B\n",
    "Images_file_after = ext_file_fake_B\n",
    "True_label_after = ext_label_fake_B\n",
    "Predic_score_after = ext_predict_score_fake_B\n",
    "Predic_class_after = ext_predict_classes_fake_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images_after.shape, Images_file_after.shape, True_label_after.shape, Predic_score_after.shape, Predic_class_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predic_score_after.shape = (1000,)\n",
    "Predic_class_after.shape = (1000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Confidential Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC CI before-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstraps = 1000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bts_scores_roc_before = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(Predic_score_before), len(Predic_score_before))\n",
    "    if len(np.unique(True_label_before[indices])) < 2:\n",
    "        continue\n",
    "    score = roc_auc_score(True_label_before[indices], Predic_score_before[indices])\n",
    "    bts_scores_roc_before.append(score)\n",
    "    \n",
    "bts_scores_roc_before = np.array(bts_scores_roc_before)\n",
    "bts_scores_roc_before.sort()\n",
    "\n",
    "ci_lower_roc_before = bts_scores_roc_before[int(0.025 * len(bts_scores_roc_before))]\n",
    "ci_upper_roc_before = bts_scores_roc_before[int(0.975 * len(bts_scores_roc_before))]\n",
    "\n",
    "roc_score_before = roc_auc_score(True_label_before, Predic_score_before)\n",
    "print(\"ROC area (before): {:0.3f}\".format(roc_score_before))\n",
    "print(\"CI for bts_scores_roc_before: [{:0.3f}-{:0.3}]\".format(ci_lower_roc_before, ci_upper_roc_before))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(bts_scores_roc_before, color='darkorange', bins=50)\n",
    "plt.plot([roc_score_before, roc_score_before], [0, 50], color='gray', linestyle=':')\n",
    "plt.title('Histogram of the ROC AUC scores \\n Bootstrapped(n=1,000) result before-GAN \\n 95% CI [{:0.3f}-{:0.3}]'.format(ci_lower_roc_before, ci_upper_roc_before), fontsize=10)\n",
    "plt.xlabel('ROC Score')\n",
    "plt.ylabel('Score Frequency')\n",
    "plt.rcParams[\"figure.figsize\"] = (4.1,2.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC CI after-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstraps = 1000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bts_scores_roc_after = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(Predic_score_after), len(Predic_score_after))\n",
    "    if len(np.unique(True_label_after[indices])) < 2:\n",
    "        continue\n",
    "    score = roc_auc_score(True_label_after[indices], Predic_score_after[indices])\n",
    "    bts_scores_roc_after.append(score)\n",
    "    \n",
    "bts_scores_roc_after = np.array(bts_scores_roc_after)\n",
    "bts_scores_roc_after.sort()\n",
    "\n",
    "ci_lower_roc_after = bts_scores_roc_after[int(0.025 * len(bts_scores_roc_after))]\n",
    "ci_upper_roc_after = bts_scores_roc_after[int(0.975 * len(bts_scores_roc_after))]\n",
    "\n",
    "roc_score_after = roc_auc_score(True_label_after, Predic_score_after)\n",
    "print(\"ROC area (after) : {:0.3f}\".format(roc_score_after))\n",
    "print(\"CI for bts_scores_roc_after: [{:0.3f}-{:0.3}]\".format(ci_lower_roc_after, ci_upper_roc_after))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(bts_scores_roc_after, color='navy', bins=50)\n",
    "plt.plot([roc_score_after, roc_score_after], [0, 50], color='skyblue', linestyle=':')\n",
    "plt.title('Histogram of the ROC AUC scores \\n Bootstrapped(n=1,000) result after-GAN \\n 95% CI [{:0.3f}-{:0.3}]'.format(ci_lower_roc_after, ci_upper_roc_after), fontsize=10)\n",
    "plt.xlabel('ROC Score')\n",
    "plt.ylabel('Score Frequency')\n",
    "plt.rcParams[\"figure.figsize\"] = (4.1,2.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(bts_scores_roc_before, color='darkorange', bins=50, label='Original-AUMC')\n",
    "plt.plot([roc_score_before, roc_score_before], [0, 50], color='gray', linestyle=':', label='Original-AUMC ROC score')\n",
    "\n",
    "plt.hist(bts_scores_roc_after, color='navy', bins=50, label='TCIAst-AUMC')\n",
    "plt.plot([roc_score_after, roc_score_after], [0, 50], color='skyblue', linestyle=':', label='TCIAst-AUMC ROC score')\n",
    "\n",
    "plt.title('Histogram of the ROC curve scores \\n Bootstrapped(n=1,000)', fontsize=15)\n",
    "plt.xlabel('ROC Score', fontsize=13)\n",
    "plt.ylabel('Score Frequency', fontsize=13)\n",
    "plt.legend(loc='upper center', fancybox=True)\n",
    "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUROC plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Before\n",
    "fpr_before, tpr_before, thresholds_before = roc_curve(True_label_before, Predic_score_before)\n",
    "roc_auc_F_before = auc(fpr_before, tpr_before)\n",
    "#label_before = \"[{:0.3f}-{:0.3}]\".format(ci_lower_roc_before, ci_upper_roc_before)\n",
    "\n",
    "# After\n",
    "fpr_after, tpr_after, thresholds_after = roc_curve(True_label_after, Predic_score_after)\n",
    "roc_auc_F_after = auc(fpr_after, tpr_after)\n",
    "#label_after = \"[{:0.3f}-{:0.3}]\".format(ci_lower_roc_after, ci_upper_roc_after)\n",
    "\n",
    "f = plt.figure()\n",
    "\n",
    "plt.title('Cancer Classification ROC curve\\n(External Validation)', fontsize=15)\n",
    "plt.xlabel('False Positive Rate(1 - Specificity)', fontsize=13)\n",
    "plt.ylabel('True Positive Rate(Sensitivity)', fontsize=13)\n",
    "\n",
    "plt.plot(fpr_after, tpr_after, 'navy', linestyle='-', \n",
    "         label='TCIAst-AUMC \\n(AUC=%0.3f [0.902-0.934])'% roc_auc_F_after)\n",
    "plt.plot(fpr_before, tpr_before, 'darkorange', linestyle='--', \n",
    "         label='Original-AUMC \\n(AUC=%0.3f [0.783-0.834])'% roc_auc_F_before)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle=':', label='Random guess')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.rcParams[\"figure.figsize\"] = (10,4.8)\n",
    "plt.style.use('default')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('/home/ssj0921/SSJ/Figures/20191203_Classification_AUROC_External_Validation.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PR CI before-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstraps = 1000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bts_scores_pr_before = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(Predic_score_before), len(Predic_score_before))\n",
    "    if len(np.unique(True_label_before[indices])) < 2:\n",
    "        continue\n",
    "    score = average_precision_score(True_label_before[indices], Predic_score_before[indices])\n",
    "    bts_scores_pr_before.append(score)\n",
    "    \n",
    "bts_score_pr_before = np.array(bts_scores_pr_before)\n",
    "bts_score_pr_before.sort()\n",
    "\n",
    "ci_lower_pr_before = bts_score_pr_before[int(0.025 * len(bts_score_pr_before))]\n",
    "ci_upper_pr_before = bts_score_pr_before[int(0.975 * len(bts_score_pr_before))]\n",
    "\n",
    "PR_score_before = average_precision_score(True_label_before, Predic_score_before)\n",
    "print(\"PR area (before) : {:0.3f}\".format(PR_score_before))\n",
    "print(\"CI for bts_score_pr_before: [{:0.3f}-{:0.3}]\".format(ci_lower_pr_before, ci_upper_pr_before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(bts_score_pr_before, color='darkorange', bins=50)\n",
    "plt.plot([PR_score_before, PR_score_before], [0, 50], color='gray', linestyle=':')\n",
    "plt.title('Histogram of the PR scores \\n Bootstrapped(n=1,000) result before-GAN \\n 95% CI [{:0.3f}-{:0.3}]'.format(ci_lower_pr_before, ci_upper_pr_before), fontsize=10)\n",
    "plt.xlabel('ROC Score')\n",
    "plt.ylabel('Score Frequency')\n",
    "plt.rcParams[\"figure.figsize\"] = (4.1,2.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PR CI after-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstraps = 1000\n",
    "rng_seed = 42  # control reproducibility\n",
    "bts_scores_pr_after = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(Predic_score_after), len(Predic_score_after))\n",
    "    if len(np.unique(True_label_after[indices])) < 2:\n",
    "        continue\n",
    "    score = average_precision_score(True_label_after[indices], Predic_score_after[indices])\n",
    "    bts_scores_pr_after.append(score)\n",
    "    \n",
    "bts_scores_pr_after = np.array(bts_scores_pr_after)\n",
    "bts_scores_pr_after.sort()\n",
    "\n",
    "ci_lower_pr_after = bts_scores_pr_after[int(0.025 * len(bts_scores_pr_after))]\n",
    "ci_upper_pr_after = bts_scores_pr_after[int(0.975 * len(bts_scores_pr_after))]\n",
    "\n",
    "PR_score_after = average_precision_score(True_label_after, Predic_score_after)\n",
    "print(\"PR area (after) : {:0.3f}\".format(PR_score_after))\n",
    "print(\"CI for bts_score_pr_after: [{:0.3f}-{:0.3}]\".format(ci_lower_pr_after, ci_upper_pr_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(bts_scores_pr_after, color='navy', bins=50)\n",
    "plt.plot([PR_score_after, PR_score_after], [0, 50], color='skyblue', linestyle=':')\n",
    "plt.title('Histogram of the PR scores \\n Bootstrapped(n=1,000) result after-GAN \\n 95% CI [{:0.3f}-{:0.3}]'.format(ci_lower_pr_after, ci_upper_pr_after), fontsize=10)\n",
    "plt.xlabel('PR Score')\n",
    "plt.ylabel('Score Frequency')\n",
    "plt.rcParams[\"figure.figsize\"] = (4.1,2.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(bts_scores_pr_before, color='darkorange', bins=50, label='Original-AUMC')\n",
    "plt.plot([PR_score_before, PR_score_before], [0, 50], color='gray', linestyle=':', label='Original-AUMC PR score')\n",
    "\n",
    "plt.hist(bts_scores_pr_after, color='navy', bins=50, label='TCIAst-AUMC')\n",
    "plt.plot([PR_score_after, PR_score_after], [0, 50], color='skyblue', linestyle=':', label='TCIAst-AUMC PR score')\n",
    "\n",
    "plt.title('Histogram of the PR curve scores \\n Bootstrapped(n=1,000)', fontsize=15)\n",
    "plt.xlabel('PR Score', fontsize=13)\n",
    "plt.ylabel('Score Frequency', fontsize=13)\n",
    "plt.legend(loc='upper center', fancybox=True)\n",
    "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR curve plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision_before = average_precision_score(True_label_before, Predic_score_before)\n",
    "average_precision_after = average_precision_score(True_label_after, Predic_score_after)\n",
    "print('Average precision-recall score: {0:0.5f}'.format(average_precision_before))\n",
    "print('Average precision-recall score: {0:0.5f}'.format(average_precision_after))\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision_before, recall_before, _ = precision_recall_curve(True_label_before, Predic_score_before)\n",
    "precision_after, recall_after, _ = precision_recall_curve(True_label_after, Predic_score_after)\n",
    "\n",
    "f = plt.figure()\n",
    "plt.step(1-recall_before, precision_before, color='darkorange', linestyle='--', alpha=1.0, where='post', \n",
    "         label='Original-AUMC \\n(area=%0.3f [0.716-0.789])'% average_precision_before)\n",
    "plt.step(1-recall_after, precision_after, color='navy', linestyle='-', alpha=1.0, where='post', \n",
    "         label='TCIAst-AUMC \\n(area=%0.3f [0.875-0.927])'% average_precision_after)\n",
    "plt.plot([0, 1], [0.513, 0.513], color='gray', linestyle=':', label='Baseline')\n",
    "\n",
    "#plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "plt.title('Cancer Classification PR curve \\n(External Validation)', fontsize=15)\n",
    "plt.xlabel('1-Recall', fontsize=13)\n",
    "plt.ylabel('Precision', fontsize=13)\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "#f.savefig('20191107_ClassExtVali-withTCIAstAUMC_PR.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of 'plot_confusion_matrix'\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix_before = confusion_matrix(True_label_before, Predic_class_before)\n",
    "cnf_matrix_after = confusion_matrix(True_label_after, Predic_class_after)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names = ['Non-cancer', 'Cancer']\n",
    "\n",
    "f1 = plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_before, classes=class_names, title='Confusion matrix (before)\\n(without normalization)')\n",
    "\n",
    "f2 = plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_before, classes=class_names, normalize=True, title='Confusion matrix (before)\\n(with normalization)')\n",
    "\n",
    "f3 = plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_after, classes=class_names, title='Confusion matrix (after)\\n(without normalization)')\n",
    "\n",
    "f4 = plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_after, classes=class_names, normalize=True, title='Confusion matrix (after)\\n(with normalization)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Diverse Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of calculating metrics for a neural network model using sklearn\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy_before = accuracy_score(True_label_before, Predic_class_before)\n",
    "accuracy_after = accuracy_score(True_label_after, Predic_class_after)\n",
    "\n",
    "# ROC AUC\n",
    "auc_before = roc_auc_score(True_label_before, Predic_score_before)\n",
    "auc_after = roc_auc_score(True_label_after, Predic_score_after)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision_before = precision_score(True_label_before, Predic_class_before)\n",
    "precision_after = precision_score(True_label_after, Predic_class_after)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall_before = recall_score(True_label_before, Predic_class_before)\n",
    "recall_after = recall_score(True_label_after, Predic_class_after)\n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1_before = f1_score(True_label_before, Predic_class_before)\n",
    "f1_after = f1_score(True_label_after, Predic_class_after)\n",
    "\n",
    "# fbeta_score\n",
    "fbeta_before = fbeta_score(True_label_before, Predic_class_before, beta=2)\n",
    "fbeta_after = fbeta_score(True_label_after, Predic_class_after, beta=2)\n",
    "\n",
    "# kappa\n",
    "kappa_before = cohen_kappa_score(True_label_before, Predic_class_before)\n",
    "kappa_after = cohen_kappa_score(True_label_after, Predic_class_after)\n",
    "\n",
    "print('Accuracy (before)    : %0.3f' % accuracy_before)\n",
    "print('ROC AUC (before)     : %0.3f' % auc_before)\n",
    "print('Precision (before)   : %0.3f' % precision_before)\n",
    "print('Recall (before)      : %0.3f' % recall_before)\n",
    "print('F1 score (before)    : %0.3f' % f1_before)\n",
    "print('fbeta(2.0) (before)  : %0.3f' % fbeta_before)\n",
    "print('Cohens kappa (before): %0.3f\\n' % kappa_before)\n",
    "\n",
    "print('Accuracy (after)     : %0.3f' % accuracy_after)\n",
    "print('ROC AUC (after)      : %0.3f' % auc_after)\n",
    "print('Precision (after)    : %0.3f' % precision_after)\n",
    "print('Recall (after)       : %0.3f' % recall_after)\n",
    "print('F1 score (after)     : %0.3f' % f1_after)\n",
    "print('fbeta(2.0) (after)   : %0.3f' % fbeta_after)\n",
    "print('Cohens kappa (after) : %0.3f' % kappa_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# Review False-positive, False-negative Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "file_class_score_df_before = pd.DataFrame({'index': sample_9056_df.filename.index.tolist(),\n",
    "                                           'filename': sample_9056_df.filename.values.tolist(),\n",
    "                                           'labeled_class': sample_9056_df.label.values.tolist(),\n",
    "                                           \n",
    "                                           'score_before': Predic_score_before.tolist(),\n",
    "                                           'class_before': Predic_class_before.tolist(),\n",
    "                                           'loss_before': sample_9056_df.label.values.tolist() - Predic_score_before,\n",
    "                                           \n",
    "                                           'score_after': Predic_score_after.tolist(),\n",
    "                                           'class_after': Predic_class_after.tolist(),\n",
    "                                           'loss_after': sample_9056_df.label.values.tolist() - Predic_score_after,\n",
    "                                           \n",
    "                                           'loss_diff': abs(sample_9056_df.label.values.tolist() - Predic_score_before) - abs(sample_9056_df.label.values.tolist() - Predic_score_after)\n",
    "                                           \n",
    "                                           })\n",
    "#len(file_class_score_df_before)\n",
    "file_class_score_df_before = file_class_score_df_before.reindex(columns=['index', 'filename', 'labeled_class',\n",
    "                                                                        'score_before', 'class_before', 'loss_before',\n",
    "                                                                        'score_after', 'class_after', 'loss_after', 'loss_diff'])\n",
    "\n",
    "loss_master = file_class_score_df_before.sort_values(by=['loss_diff'], axis=0, ascending=[False])\n",
    "loss_master.to_csv('./loss_master.csv')\n",
    "loss_master.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False-positive 중에 loss가 크게 개선된 순서대로 Image-set 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_master['group'] = 0\n",
    "\n",
    "loss_master.loc[(loss_master['labeled_class']==0) &\n",
    "                (loss_master['class_before']==1) &\n",
    "                (loss_master['class_after']==0), 'group'] = 'only_before_fp'\n",
    "\n",
    "loss_master.loc[(loss_master['labeled_class']==1) &\n",
    "                (loss_master['class_before']==0) &\n",
    "                (loss_master['class_after']==1), 'group'] = 'only_before_fn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_before_fp_df = loss_master[loss_master.group == 'only_before_fp']\n",
    "len(loss_master[loss_master.group == 'only_before_fp']) # 295\n",
    "only_before_fp_df[only_before_fp_df.group == 'only_before_fp'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_before_fp_files = only_before_fp_df.filename.values.tolist()\n",
    "only_before_fp_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## false positive\n",
    "\n",
    "only_before_fp_files = only_before_fp_df.filename.values.tolist()\n",
    "\n",
    "# len(only_before_fp_df)\n",
    "\n",
    "for i in range(len(only_before_fp_df)):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    name = only_before_fp_files[i]\n",
    "    diff = only_before_fp_df.loss_diff.values[i]\n",
    "    \n",
    "    fig.add_subplot(1,2,1)\n",
    "    path_b = '/home/ssj0921/SSJ/Data/20191203_Classification/20191203_real_A_2000/' + name + '_real_A.png'\n",
    "    score_b = only_before_fp_df.score_before.values[i]\n",
    "    image_b = imread(path_b)\n",
    "    plt.imshow(image_b)\n",
    "    plt.axis('off')\n",
    "    plt.title('False-positive\\n(score=%0.3f)' % (score_b), fontsize=13)\n",
    "    \n",
    "    fig.add_subplot(1,2,2)\n",
    "    path_a = '/home/ssj0921/SSJ/Data/20191203_Classification/20191203_fake_B_2000/' + name + '_fake_B.png'\n",
    "    score_a = only_before_fp_df.score_after.values[i]\n",
    "    image_a = imread(path_a)\n",
    "    plt.imshow(image_a)\n",
    "    plt.axis('off')\n",
    "    plt.title('True-negative\\n(score=%0.3f)' % (score_a), fontsize=13)    \n",
    "    \n",
    "    #plt.rcParams[\"figure.figsize\"] = (3,3)    \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    #plt.show()   \n",
    "    \n",
    "    plt.savefig('/home/ssj0921/SSJ/Results/20191203_ImageSet/only_before_fp/[%0.3f]_%s.png' % (diff, name), dpi=500,\n",
    "               bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False-negative 중에 loss가 크게 개선된 순서대로 Image-set 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_before_fn_df = loss_master[loss_master.group == 'only_before_fn']\n",
    "len(loss_master[loss_master.group == 'only_before_fn']) # 2\n",
    "only_before_fn_df[only_before_fn_df.group == 'only_before_fn'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_before_fn_files = only_before_fn_df.filename.values.tolist()\n",
    "only_before_fn_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## false negative\n",
    "\n",
    "only_before_fn_files = only_before_fn_df.filename.values.tolist()\n",
    "\n",
    "# len(only_before_fp_df)\n",
    "\n",
    "for i in range(len(only_before_fn_df)):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    name = only_before_fn_files[i]\n",
    "    diff = only_before_fn_df.loss_diff.values[i]\n",
    "    \n",
    "    fig.add_subplot(1,2,1)\n",
    "    path_b = '/home/ssj0921/SSJ/Data/20191203_Classification/20191203_real_A_2000/' + name + '_real_A.png'\n",
    "    score_b = only_before_fn_df.score_before.values[i]\n",
    "    image_b = imread(path_b)\n",
    "    plt.imshow(image_b)\n",
    "    plt.axis('off')\n",
    "    plt.title('False-negative\\n(score=%0.3f)' % (score_b), fontsize=13)\n",
    "    \n",
    "    fig.add_subplot(1,2,2)\n",
    "    path_a = '/home/ssj0921/SSJ/Data/20191203_Classification/20191203_fake_B_2000/' + name + '_fake_B.png'\n",
    "    score_a = only_before_fn_df.score_after.values[i]\n",
    "    image_a = imread(path_a)\n",
    "    plt.imshow(image_a)\n",
    "    plt.axis('off')\n",
    "    plt.title('True-positive\\n(score=%0.3f)' % (score_a), fontsize=13)    \n",
    "    \n",
    "    #plt.rcParams[\"figure.figsize\"] = (3,3)    \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    #plt.show()   \n",
    "    \n",
    "    plt.savefig('/home/ssj0921/SSJ/Results/20191203_ImageSet/only_before_fn/[%0.3f]_%s.png' % (diff, name), dpi=500,\n",
    "               bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False-positive 중에 cGAN 후에도 여전히 False-positive인 Image-set 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loss_master[loss_master.group == 0]) # 711\n",
    "#loss_master[loss_master.group == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_master.loc[(loss_master['labeled_class']==0) &\n",
    "                (loss_master['class_before']==1) &\n",
    "                (loss_master['class_after']==1), 'group'] = 'always_fp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#always_fp_df = loss_master[loss_master.group == 'always_fp']\n",
    "#len(loss_master[loss_master.group == 'always_fp']) # 166\n",
    "always_fp_df[always_fp_df.group == 'always_fp'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "always_fp_files = always_fp_df.filename.values.tolist()\n",
    "always_fp_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## false positive\n",
    "\n",
    "always_fp_files = always_fp_df.filename.values.tolist()\n",
    "\n",
    "# len(always_fp_df)\n",
    "\n",
    "for i in range(len(always_fp_df)):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    name = always_fp_files[i]\n",
    "    diff = always_fp_df.loss_diff.values[i]\n",
    "    \n",
    "    fig.add_subplot(1,2,1)\n",
    "    path_b = '/home/ssj0921/SSJ/Data/20191203_Classification/20191203_real_A_2000/' + name + '_real_A.png'\n",
    "    score_b = always_fp_df.score_before.values[i]\n",
    "    image_b = imread(path_b)\n",
    "    plt.imshow(image_b)\n",
    "    plt.axis('off')\n",
    "    plt.title('False-positive\\n(score=%0.3f)' % (score_b), fontsize=13)\n",
    "    \n",
    "    fig.add_subplot(1,2,2)\n",
    "    path_a = '/home/ssj0921/SSJ/Data/20191203_Classification/20191203_fake_B_2000/' + name + '_fake_B.png'\n",
    "    score_a = always_fp_df.score_after.values[i]\n",
    "    image_a = imread(path_a)\n",
    "    plt.imshow(image_a)\n",
    "    plt.axis('off')\n",
    "    plt.title('False-positive\\n(score=%0.3f)' % (score_a), fontsize=13)    \n",
    "    \n",
    "    #plt.rcParams[\"figure.figsize\"] = (3,3)    \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    #plt.show()   \n",
    "    \n",
    "    # '/home/ssj0921/SSJ/Results/loss_20191203/only_before_fp/[%0.3f]_%s.png'\n",
    "    plt.savefig('/home/ssj0921/SSJ/Results/20191203_ImageSet/always_fp/[%0.3f]_%s.png' % (diff, name), dpi=500,\n",
    "               bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False-negative 중에 cGAN 후에도 여전히 False-negative인 Image-set 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_master.loc[(loss_master['labeled_class']==1) &\n",
    "                (loss_master['class_before']==0) &\n",
    "                (loss_master['class_after']==0), 'group'] = 'always_fn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#always_fn_df = loss_master[loss_master.group == 'always_fn']\n",
    "len(loss_master[loss_master.group == 'always_fn']) # 166\n",
    "#always_fn_df[always_fn_df.group == 'always_fn'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "always_fn_files = always_fn_df.filename.values.tolist()\n",
    "always_fn_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## false positive\n",
    "\n",
    "always_fn_files = always_fn_df.filename.values.tolist()\n",
    "\n",
    "# len(always_fp_df)\n",
    "\n",
    "for i in range(len(always_fn_df)):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    name = always_fn_files[i]\n",
    "    diff = always_fn_df.loss_diff.values[i]\n",
    "    \n",
    "    fig.add_subplot(1,2,1)\n",
    "    path_b = '/home/ssj0921/SSJ/Data/20191203_Classification/20191203_real_A_2000/' + name + '_real_A.png'\n",
    "    score_b = always_fn_df.score_before.values[i]\n",
    "    image_b = imread(path_b)\n",
    "    plt.imshow(image_b)\n",
    "    plt.axis('off')\n",
    "    plt.title('False-negative\\n(score=%0.3f)' % (score_b), fontsize=13)\n",
    "    \n",
    "    fig.add_subplot(1,2,2)\n",
    "    path_a = '/home/ssj0921/SSJ/Data/20191203_Classification/20191203_fake_B_2000/' + name + '_fake_B.png'\n",
    "    score_a = always_fn_df.score_after.values[i]\n",
    "    image_a = imread(path_a)\n",
    "    plt.imshow(image_a)\n",
    "    plt.axis('off')\n",
    "    plt.title('False-negative\\n(score=%0.3f)' % (score_a), fontsize=13)    \n",
    "    \n",
    "    #plt.rcParams[\"figure.figsize\"] = (3,3)    \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    #plt.show()   \n",
    "    \n",
    "    # '/home/ssj0921/SSJ/Results/loss_20191203/only_before_fp/[%0.3f]_%s.png'\n",
    "    plt.savefig('/home/ssj0921/SSJ/Results/20191203_ImageSet/always_fn/[%0.3f]_%s.png' % (diff, name), dpi=500,\n",
    "               bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
